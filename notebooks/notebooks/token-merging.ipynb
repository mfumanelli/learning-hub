{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecba4260-ba9d-49d9-9409-87712c9c5084",
   "metadata": {},
   "source": [
    "![Tome](../imgs/tome.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c862c2f-c30e-4c65-9956-1b878c18bff6",
   "metadata": {},
   "source": [
    "<center><small style=\"font-size: 12px;\">Image taken from Meta's blog: \"Token Merging: Your ViT but faster\".</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432eab25-d14d-46dd-9f2c-51326fdf3db7",
   "metadata": {},
   "source": [
    "<h1>Token Merging (ToMe)‚Ää-‚ÄäMeta AI's New Optimization Technique to Make ViT Faster by 2x. But Can ViT be Even¬†Faster?</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb5c7d-0400-487a-b17b-3cd90beb2fa0",
   "metadata": {},
   "source": [
    "<h2>Playing with ToMe and benchmarking it against other inference optimization strategies</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584946a1-be39-4f03-9cfa-878aa0921365",
   "metadata": {},
   "source": [
    "The goal of this notebook is to explore Meta Research's new Token Merging (ToMe) optimization strategy, perform some practical experiments with it, and benchmark ToMe with other state-of-the-art inference optimization techniques with the opensource library Speedster.\n",
    "We will try to answer a few questions:\n",
    "* What's the accuracy-latency trade-off with ToMe?\n",
    "* Can we reproduce Meta's results?\n",
    "* Meta tested Tome performances with an optimal batch size on a GPU V100. How does ToMe perform with different batch sizes and on cpus?\n",
    "* How does ToMe perform compared to other optimization techniques, such as quantization or pruning and compilation?\n",
    "* Can ToMe be combined with other optimization strategies to achieve a multiplier effect on throughput?\n",
    "* Meta's images are super cool üôå. How can you ToMe-nize one of your pictures?\n",
    "\n",
    "You can also find a [blog post](https://www.nebuly.com/blog/token-merging-tome-meta-ais-new-optimization-technique-to-make-vit-faster) where we discuss all these points with less code üåà\n",
    "\n",
    "Let's first explore ToMe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee93054-48e4-4404-bce1-5cd464d3c52d",
   "metadata": {},
   "source": [
    "<h2>üß© ToMe: Token Merging</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd0c60f-1cbe-4788-b29f-9d6f02c39073",
   "metadata": {},
   "source": [
    "<b>Token Merging (ToMe)</b> is a technique recently introduced by Meta AI to reduce the latency of existing Vision Transformer (ViT) models without the need for additional training. ToMe gradually combines similar tokens into a transformer using an algorithm as lightweight as pruning while maintaining better accuracy.\n",
    "\n",
    "ToMe introduces a module for token merging into an existing ViT, merging redundant tokens to improve both inference and training throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec480b0-7e45-4b9a-80d9-a73bd810c2aa",
   "metadata": {},
   "source": [
    "![tomeintro](../imgs/tomeintro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2ecd0-dae5-4f3b-a200-14e83110c928",
   "metadata": {},
   "source": [
    "<center><small style=\"font-size: 12px;\">ToMe accuracy vs inference speed performance. Image from Meta's blog: \"Token Merging: Your ViT but faster\".</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a961022-42ac-4522-91a7-b0b182363d0f",
   "metadata": {},
   "source": [
    "<h3>üó∫Ô∏è Optimization Strategy</h3>\n",
    "ViT converts image patches into \"tokens\". Then, it applies an attention mechanism to each layer that allows these tokens to collect information from one another proportionally to their similarity. To improve the speed of ViT while maintaining its accuracy, ToMe builds on two observations:\n",
    "\n",
    "1. Computation speed and memory use are heavily tied to the number of tokens in the transformer\n",
    "2. These tokens are often redundant\n",
    "\n",
    "In each transformer block, tokens are combined (and thus reduced in number) by a quantity r of tokens per layer. Over the L blocks in the network, a number of rL tokens are merged. By varying the parameter r, we get a speed-accuracy trade-off, as fewer tokens means lower accuracy but higher throughput.\n",
    "\n",
    "The image below shows how the token merging step is applied between the attention and MLP branches of each transformer block. Step by step, the dog's fur is merged into a single token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a19469-5aa9-44cd-a23c-b1fcd3f07c89",
   "metadata": {},
   "source": [
    "![Tome-arch](../imgs/tome-arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4fe5c-2b60-4880-8d87-dfe9eb0188f3",
   "metadata": {},
   "source": [
    "<center><small style=\"font-size: 12px;\">Image taken from Meta's paper: \"Token Merging: Your ViT but faster\".</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d3b90-ebfc-469a-9a1b-3346456968c5",
   "metadata": {},
   "source": [
    "<h3>üßë‚Äçü§ù‚Äçüßë The Concept of Similarity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b18d81-41fe-4e9e-b679-0a1de768c0af",
   "metadata": {},
   "source": [
    "ToMe reduces the number of tokens by combining similar ones. The similarity between tokens is defined using self-attention QVKs. Specifically, the keys (K) summarise the information contained in each token. A dot product similarity metric (e.g. cosine similarity) between the keys of each token is then defined as a metric that measures the similarity between the different tokens, in order to understand whether they contain similar information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284dc16-7934-4848-be99-7baaefe1951f",
   "metadata": {},
   "source": [
    "<h3>üìà Paper Results</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c663ef03-40a1-41fa-8ee0-c38ff0e65a8a",
   "metadata": {},
   "source": [
    "So, what's the accuracy-latency trade-off with ToMe?\n",
    "\n",
    "Let's have a look at the results reported in the paper, which were obtained on a V100 GPU. I plotted the accuracy of ViT as a function of the hyperparameter r, where the original ViT corresponds to r=0. We can see that smaller values of r correspond to a model that is slower but with accuracy more faithful to the original model. Large values of r result in a considerable acceleration of the model but a loss in accuracy. For instance, to achieve 2x acceleration from the original model, the model loses 4 points of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497347fd-a9d0-4142-baee-2e1b3aaf01e2",
   "metadata": {},
   "source": [
    "![tomeres](../imgs/infer-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29632436-707b-4764-93c1-675fbc56de7f",
   "metadata": {},
   "source": [
    "<center><small style=\"font-size: 12px;\">Results taken from Meta's paper: \"Token Merging: Your ViT but faster\".</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41080b95-36e6-4f1c-b022-faba64638678",
   "metadata": {},
   "source": [
    "All results shown are for the ViT-B/16 model, which is also the model that will be used in the notebook for the various experiments as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df842a06-1457-405d-9723-fe35cc96d89a",
   "metadata": {},
   "source": [
    "<h2>üë∑‚Äç‚ôÄÔ∏è Hands-on</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898296f8-0d13-473c-b79a-4d266c0ed7e0",
   "metadata": {},
   "source": [
    "Let's see if we can reproduce Meta's results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1aca69-8239-4092-b4d4-17f28b8fb58b",
   "metadata": {},
   "source": [
    "<h3>‚öôÔ∏è Libraries Intallation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44129fc7-a550-4389-a9b6-be52e0a4d602",
   "metadata": {},
   "source": [
    "<h4>ToMe Installation</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0564c5c-6d75-4e26-ba06-8e02ac450cf6",
   "metadata": {},
   "source": [
    "To install ToMe, follow the [instructions](https://github.com/facebookresearch/ToMe/blob/main/INSTALL.md) released with the implementation by Meta Research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da853d-80b3-4ce0-8c0f-a589ec66998b",
   "metadata": {},
   "source": [
    "<h4>Speedster Installation</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42895061-1e00-4730-bd9a-606e07b16531",
   "metadata": {},
   "source": [
    "Install Speedster and its base requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252af3bd-a599-4da3-bda4-efc5609fd9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install speedster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac975f-43ba-43a2-801c-74545b14e76c",
   "metadata": {},
   "source": [
    "Then make sure to install all the available deep learning compilers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e0714-9b48-4245-93f2-92066b153266",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nebullvm.installers.auto_installer --compilers all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0c053-6f98-4822-a527-5e3320375c91",
   "metadata": {},
   "source": [
    "<h4>Timm Installation</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96fbf96-4bb7-46a3-82b0-bb6f01e13596",
   "metadata": {},
   "source": [
    "Timm exists only in pre-release version, so to install it run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b072909-2d5b-41b1-bfd5-c2c2eac4619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9063d0b3-eb23-4e0b-93fc-583f7b225976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from speedster import optimize_model, save_model, load_model\n",
    "\n",
    "from nebullvm.tools.benchmark import benchmark\n",
    "\n",
    "import timm, tome\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be55a7-c99a-4c17-93e3-14f897f1f31f",
   "metadata": {},
   "source": [
    "<h3>üî• ToMe Optimization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f47130-7cd9-46f7-8fe7-e8aea29db5c5",
   "metadata": {},
   "source": [
    "<h4>üë©‚Äçüî¨ Experiments</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dc200d-94b1-4180-bbcd-f1e271e1fecb",
   "metadata": {},
   "source": [
    "I ran the experiments on ViT-B/16 model on a GPU V100 with batch size 64 as in the paper, the recommended value of the hyperparameter r=16. Then, I tested ToMe on smaller batch sizes up to batch size=1, and replicated the same experiment on a CPU E5‚Äì2686. A p3.2xlarge instance of aws was used for all experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d6fda-e02c-4a2a-a595-efc37f7e6781",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 1</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcde7c-4ea5-4c40-8ccc-fc6e0e3469a4",
   "metadata": {},
   "source": [
    "<h4>Original model benchmark - batch size = 1</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49afeba-7bdb-43ef-9c49-cd0a75c3b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(1, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548fe0d3-d512-437e-ae6b-95a000aaced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af1ba0-2103-4da3-90ea-0a791877e2ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641e1b23-81c0-4a39-875c-e98947c900a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:46:47\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:05<00:00,  9.66it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:08<00:00, 113.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Average Throughput: 114.39 data/second\n",
      "Average Latency: 0.0087 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd66c3-e568-480d-bade-8078f57c8cbd",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44976a1-958a-4cac-b059-664540aba231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:47:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 20 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.37it/s]\n",
      "Performing benchmark on 100 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:17<00:00,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Average Throughput: 5.60 data/second\n",
      "Average Latency: 0.1784 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=20, n_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf74ebe-35f4-4513-9ed2-1e217845c426",
   "metadata": {},
   "source": [
    "<h4> ToMe Optimization </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f1caf81-37c9-4088-8aab-c566d96ea88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tome.patch.timm(model)\n",
    "# Set the number of tokens reduced per layer\n",
    "model.r = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e9c0e8-b892-4d7e-b55d-5a3ffd523fc8",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimized Model Benchmark - batch size = 1</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea6e45-69be-4daa-a96f-cae824d981c8",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63b86aed-d2e4-4ea1-8417-6497a2dfefa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:47:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 50.34it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:18<00:00, 53.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Average Throughput: 53.50 data/second\n",
      "Average Latency: 0.0187 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eac1d4-e34b-4c4b-9189-d92f54a705fb",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a8a1e78-abea-4a3b-8cfc-e9250b461761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:48:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 20 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  8.31it/s]\n",
      "Performing benchmark on 100 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:11<00:00,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Average Throughput: 8.69 data/second\n",
      "Average Latency: 0.1151 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=20, n_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f4927-3293-4044-99fd-9de92c1d03c5",
   "metadata": {},
   "source": [
    "The original model with the batch size of 1 has a throughput of <b>114.39 data/second</b> with a latency of <b>0.0087 seconds/data</b> on gpu, while on cpu: <b>5.60 data/second</b> and <b>0.1784 seconds/data</b> latency. After optimizing the model with ToMe we get a throughput of <b>53.50 data/second</b> and latency <b>0.0187 seconds/data</b> on gpu while <b>8.69 data/second</b> and <b>0.1151 seconds/data</b> on cpu. \n",
    "\n",
    "This means that the ToMe optimization library in inference for cases where the batch size is equal to one (a very common case when in inference) slows down the model, almost doubling its time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23af68-10c5-45df-918d-059a28e0c97c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 2</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b40795-57d6-4fbd-849b-e30ba136d644",
   "metadata": {},
   "source": [
    "<h4>Original model benchmark - batch size = 2</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb2a2325-56b6-4bbe-8f7c-51fabe4b5070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(2, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9aa3632-a6d4-4f34-b46d-19a25084b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3220a8-0fd2-452c-8995-4f904e52b86f",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b4e0db-f51e-4ace-9da9-8841cd1b5303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:48:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 109.44it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:09<00:00, 107.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n",
      "Average Throughput: 216.19 data/second\n",
      "Average Latency: 0.0046 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388c14a-31d2-4f7f-9131-f50c184eb98d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b995d6e-ca10-4ea9-9ad2-a55a4210f78e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:48:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 20 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.01it/s]\n",
      "Performing benchmark on 100 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:33<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n",
      "Average Throughput: 6.03 data/second\n",
      "Average Latency: 0.1660 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=20, n_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e800a33-4cce-4270-93fd-cd8209e4ca85",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31999098-29a0-4adb-8bce-173345ccaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tome.patch.timm(model)\n",
    "# Set the number of tokens reduced per layer\n",
    "model.r = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a297a50-1bd2-46d7-b09e-d108dfb89999",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimized Model Benchmark - batch size = 2</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07379629-fd42-46db-aed5-4fa5d3f15a7d",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4910fc3c-6084-4069-b8e5-66f6b9353450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:49:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 50.92it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:19<00:00, 50.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n",
      "Average Throughput: 102.59 data/second\n",
      "Average Latency: 0.0097 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57677de0-2fc8-4c7b-97c4-080f878ba4c5",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e97c9149-ae24-4ff2-9ca6-17373109f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:49:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 20 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.96it/s]\n",
      "Performing benchmark on 100 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:19<00:00,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n",
      "Average Throughput: 10.16 data/second\n",
      "Average Latency: 0.0984 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=20, n_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85269f1-fc81-4f9a-b817-2a19059099d0",
   "metadata": {},
   "source": [
    "In this case the original model with the batch size of 2 has a throughput of <b>216.19 data/second</b> with a latency of <b>0.0046 seconds/data</b> on gpu, while on cpu: <b>6.03 data/second</b> and <b>0.1660 seconds/data</b> latency. After optimizing the model with ToMe we get a throughput of <b>102.59 data/second</b> and latency <b>0.0097 seconds/data</b> on gpu while <b>10.16 data/second</b> and <b>0.0984 seconds/data</b> on cpu. \n",
    "\n",
    "Here we can see how ToMe speeds up the model in inference on cpu, where the impact of batch size is limited, while it still slows down the model on gpu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da0cd6-ca3d-4287-a368-439a2a028763",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 4</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d81e58-44d7-46cc-b507-ba2fa3699bd3",
   "metadata": {},
   "source": [
    "<h4>Original model benchmark - batch size = 4</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bd5f2c6-81ef-42bc-bfdd-2267444249f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(4, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5d397b9-2d0f-4b89-bc52-37a16867a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0765e254-8ea1-4e9f-ad39-ad1532b266fa",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a688eadf-a151-4dd1-ad7d-5d41bc6800c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:50:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 82.22it/s] \n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:13<00:00, 72.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Average Throughput: 292.85 data/second\n",
      "Average Latency: 0.0034 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc242177-4261-4c8c-97a4-acdf9ac86ec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81173a21-402f-4ba7-9e25-7e9c9a5bb57e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:50:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 20 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:11<00:00,  1.71it/s]\n",
      "Performing benchmark on 100 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:58<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Average Throughput: 6.86 data/second\n",
      "Average Latency: 0.1457 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=20, n_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32de001-4898-4e09-8e69-7ea7ccfb7da0",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7df5ee8-d9d8-4653-9dd8-9b5073fa6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tome.patch.timm(model)\n",
    "# Set the number of tokens reduced per layer\n",
    "model.r = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac47f6f0-975d-48ef-b4cc-518aae78ccdf",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimized Model Benchmark - batch size = 4</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a82e0-fbe0-4f46-b107-2c30b64b78d0",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29fe8b5a-de8b-4993-ae84-be7e2dac04e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:51:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 50.34it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:19<00:00, 50.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Average Throughput: 202.81 data/second\n",
      "Average Latency: 0.0049 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894c535-ecc8-4d58-a1cd-cbc336bd4905",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c7a7d18-204d-49bd-bed9-2c470faf63c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:52:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 20 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:08<00:00,  2.46it/s]\n",
      "Performing benchmark on 100 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Average Throughput: 11.81 data/second\n",
      "Average Latency: 0.0847 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=20, n_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127a36c-30b7-4179-88bd-45ffd80657ed",
   "metadata": {},
   "source": [
    "The original model with the batch size of 4 has a throughput of <b>292.85 data/second</b> with a latency of <b>0.0034 seconds/data</b> on gpu, while on cpu: <b>6.86 data/second</b> and <b>0.1457 seconds/data</b> latency. After optimizing the model with ToMe we get a throughput of <b>202.81 data/second</b> and latency <b>0.0049 seconds/data</b> on gpu while <b>11.81 data/second</b> and <b>0.0847 seconds/data</b> on cpu.\n",
    "\n",
    "Also in this case ToMe slows down the model when used on gpu while giving it a boost on cpu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026d2e4-726f-45ac-b410-983390bde871",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 8</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b3b72-e58c-41e3-a76a-b8de57efdfee",
   "metadata": {},
   "source": [
    "<h4>Original model benchmark - batch size = 8</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "554d7e4c-e528-483a-9227-40b42a1841c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(8, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d810660b-9ba6-44dd-b416-326e94500e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e568364-1ba5-4aac-b1ac-1a2fe7a28539",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6fc93dc1-f145-4bd9-912e-658773df68c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:52:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 35.13it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:26<00:00, 38.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Average Throughput: 307.51 data/second\n",
      "Average Latency: 0.0033 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b3436-d003-47d8-9b25-4bba1ee6c8b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b93a821-1684-466c-ae0f-37f07eb510a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:53:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 20 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:22<00:00,  1.15s/it]\n",
      "Performing benchmark on 100 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:55<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Average Throughput: 6.96 data/second\n",
      "Average Latency: 0.1438 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=20, n_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a738983-6beb-422c-9308-aebc248dd499",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92b018b0-cbfe-4ff4-8c02-567bbdf687fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tome.patch.timm(model)\n",
    "# Set the number of tokens reduced per layer\n",
    "model.r = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565a575-00e3-4e6d-a8dc-2073605b70f0",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimized Model Benchmark - batch size = 8</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56553bf-ed6b-45ee-ba00-22b660ce9cc3",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5838747a-3e36-4dda-9877-67e381f2992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:55:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 50.07it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:19<00:00, 50.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Average Throughput: 405.17 data/second\n",
      "Average Latency: 0.0025 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a2f8b-dbfd-443e-abf7-eb096954b1db",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50a0021b-5469-43fb-908a-48a433b22a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:56:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 20 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:12<00:00,  1.64it/s]\n",
      "Performing benchmark on 100 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:02<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Average Throughput: 12.82 data/second\n",
      "Average Latency: 0.0780 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=20, n_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ce4b5-c8d9-47c5-9028-b68c99bf8b6d",
   "metadata": {},
   "source": [
    "The original model with the batch size of 8 has a throughput of <b>307.51 data/second</b> with a latency of <b>0.0033 seconds/data</b> on gpu, while on cpu: <b>6.96 data/second</b> and <b>0.1438 seconds/data</b> latency. After optimizing the model with ToMe we get a throughput of <b>405.17 data/second</b> and latency <b>0.0025 seconds/data</b> on gpu while <b>12.82 data/second</b> and <b>0.0780 seconds/data</b> on cpu.\n",
    "\n",
    "Here one can see the first speed improvement in both cpu and gpu use cases after applying ToMe to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75678ecf-ddee-429d-aa47-dc951cf77647",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 32</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b74b8-51f1-496e-8425-d5e8cea7dfe6",
   "metadata": {},
   "source": [
    "<h4>Original model benchmark - batch size = 32</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c8cb305-9d51-40cc-ad96-6e3ed7e37345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(32, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db9de14a-a173-4812-bbd4-2d08ed698b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73478b1-55e9-4553-b1a0-b80df596990c",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f41f17d3-0ded-41a3-81fd-dda42e01e71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 08:57:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:04<00:00, 11.08it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:40<00:00,  9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Average Throughput: 319.47 data/second\n",
      "Average Latency: 0.0031 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3ad4d-6831-43be-81ff-98f03337daf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2311b0f5-cb69-41a5-ada9-2057d3b652dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:00:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 10 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:50<00:00,  5.06s/it]\n",
      "Performing benchmark on 15 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:13<00:00,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Average Throughput: 6.50 data/second\n",
      "Average Latency: 0.1539 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=10, n_runs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a145d4cb-1898-46a0-88f0-39d602adea19",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "558f0884-112e-4b97-92a8-84ba9cfdf965",
   "metadata": {},
   "outputs": [],
   "source": [
    "tome.patch.timm(model)\n",
    "# Set the number of tokens reduced per layer\n",
    "model.r = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f530fecd-c213-46c6-99ad-34aab3d35093",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimized Model Benchmark - batch size = 32</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc94aa8-4c21-4708-878c-5f63c88d721d",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7be0468f-b9be-47a4-9b82-c15d8df78bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:02:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:02<00:00, 19.19it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:54<00:00, 18.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Average Throughput: 589.79 data/second\n",
      "Average Latency: 0.0017 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b5802-3c78-48a8-a623-191b6edcba54",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b85338d5-ece6-4ad5-a6f0-3aac1b00d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:03:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 10 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:24<00:00,  2.47s/it]\n",
      "Performing benchmark on 15 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:39<00:00,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Average Throughput: 12.07 data/second\n",
      "Average Latency: 0.0828 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=10, n_runs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6181137-1a2b-47e2-9bd6-14c3813a12ab",
   "metadata": {},
   "source": [
    "In this case, with batch size of 32, the original model has a throughput of <b>319.47 data/second</b> with a latency of <b>0.0031 seconds/data</b> on gpu, while on cpu: <b>6.50 data/second</b> and <b>0.1539 seconds/data</b> latency. After optimizing the model with ToMe we get a throughput of <b>589.79 data/second</b> and latency <b>0.0017 seconds/data</b> on gpu while <b>12.07 data/second</b> and <b>0.0828 seconds/data</b> on cpu.\n",
    "\n",
    "As the batch size increases, one can see the improvement in model speed achieved through the application of ToMe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62c066-81a1-45a3-9a10-317b6ea8c50e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 64</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f5ad5-530d-4447-839f-643c76cba601",
   "metadata": {},
   "source": [
    "<h4>Original model benchmark - batch size = 64</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "272f91d8-558e-4cb5-b333-0434796d22a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(64, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52da01d3-e237-4171-ac2f-ec236e7b0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343a762-cc93-4e9b-b965-0e155a4e3be4",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cce02766-a60a-4c17-89f6-5bc71c04c180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:04:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:08<00:00,  5.80it/s]\n",
      "Performing benchmark on 500 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [01:36<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Average Throughput: 333.49 data/second\n",
      "Average Latency: 0.0030 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, n_runs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb39c5-f535-48d2-86ee-e03f52a5c632",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7205be40-34ac-4c10-af08-0de935b3632c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:06:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 10 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:01<00:00, 12.15s/it]\n",
      "Performing benchmark on 15 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [02:57<00:00, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Average Throughput: 5.40 data/second\n",
      "Average Latency: 0.1850 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=10, n_runs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6323c205-f243-4848-be37-fdd93542f7c7",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimization</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6ad5f36-ad27-4dce-a19b-2613e5bfeb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tome.patch.timm(model)\n",
    "# Set the number of tokens reduced per layer\n",
    "model.r = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2923b83-2c09-4f79-a666-6462f4b26cf4",
   "metadata": {},
   "source": [
    "<h4>ToMe Optimized Model Benchmark - batch size = 64</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8da83b-ebc8-4825-ab42-dde8452eead9",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcdf99fb-06a3-4533-8a5c-2f83e6a8d8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:11:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:04<00:00, 10.18it/s]\n",
      "Performing benchmark on 500 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:51<00:00,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Average Throughput: 625.73 data/second\n",
      "Average Latency: 0.0016 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, n_runs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ab6f2e-9a16-4e56-b696-c0c51776f080",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a94b0603-217a-4987-ac5c-505ea4654561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:12:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 10 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:56<00:00,  5.70s/it]\n",
      "Performing benchmark on 15 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:25<00:00,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Average Throughput: 11.23 data/second\n",
      "Average Latency: 0.0891 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, input_data, device=\"cpu\", n_warmup=10, n_runs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1843745-1397-4160-aa7d-41ae72a4702f",
   "metadata": {},
   "source": [
    "In this final case, with batch size of 64, the original model has a throughput of <b>333.49 data/second</b> with a latency of <b>0.0030 seconds/data</b> on gpu, while on cpu: <b>5.40 data/second</b> and <b>0.1850 seconds/data</b> latency. After optimizing the model with ToMe we get a throughput of <b>625.73 data/second</b> and latency <b>0.0016 seconds/data</b> on gpu while <b>11.23 data/second</b> and <b>0.0891 seconds/data</b> on cpu.\n",
    "\n",
    "This is the batch size used as the default within the library, here you can see a <b>2x</b> in speed for the optimized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8871ff2-a9e0-4667-99e0-5c44aedfa4bf",
   "metadata": {},
   "source": [
    "The next step is to apply Speedster to the chosen model, again on both cpu and gpu and with different batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b3f7a-f099-42c0-aea9-cbf22c50bcf1",
   "metadata": {},
   "source": [
    "<h3>üëæ Results</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb1743-3a85-4a2b-9cbb-f59dba70c74f",
   "metadata": {},
   "source": [
    "The results are very interesting:\n",
    "\n",
    "* The use of ToMe is very simple, and we are able to reproduce Meta's acceleration of 2x on a V100 for batch size 64.\n",
    "* On GPU, ToMe is very sensitive to the batch size, and for low batch sizes it produces poorer performances with respect to the original model. This is because in this case the resources are not fully used when the batch size is small, in this case the highly parallelizable GPU still has space for further parallel computation. The token-reduction becomes significant on GPU only when the GPU power is fully utilized.\n",
    "* On CPU, performances are around 2x both for very small and larger batch sizes. This is due to the fact that also for smaller batch sizes the CPU compute power is fully used by the network. Thus, the overhead of ToMe on CPU can already be compensated by the token-reduction for smaller batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c2ced7-f168-41bb-9cdc-554b508853b2",
   "metadata": {},
   "source": [
    "![tomeresults](../imgs/tome-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc996d8-0c24-488a-b1dd-1d091846b805",
   "metadata": {},
   "source": [
    "<center><small style=\"font-size: 12px;\">Throughput graph for the original model and the model to which ToMe was applied, as the batch size varies.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a066e3c2-6718-4229-bc01-535a56262b62",
   "metadata": {},
   "source": [
    "<h2>üöÄ ToMe vs Other Optimization Techniques</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3914e13-3ddb-4247-92a9-2a757c01c568",
   "metadata": {},
   "source": [
    "As a next step, I compared the performance of ToMe with what can be achieved by other optimization techniques. I used the Speedster library to run the optimizations and see the performance on CPU and GPU.\n",
    "\n",
    "<b>Speedster</b> is an open-source module designed to speed up AI inference in just a few lines of code. Its use is very simple. The library automatically applies the best set of SOTA optimization techniques to achieve the maximum inference speed-up (of latency and throughput, while compressing the model size) physically possible on the available hardware.\n",
    "\n",
    "The optimization workflow consists of 3 steps: select, search, and serve.\n",
    "\n",
    "üìö <b>Select step</b>: in this step, users input their model in their preferred deep learning framework and express their preferences regarding maximum consented accuracy loss and optimization time. This information is used to guide the optimization process and ensure that the resulting model meets the user's needs.\n",
    "\n",
    "üîç <b>Search step</b>: the library automatically tests every combination of optimization techniques across the software-to-hardware stack, such as sparsity, quantization, and compilers, that is compatible with the user's preferences and local hardware. This allows the library to find the optimal configuration of techniques for accelerating the model.\n",
    "\n",
    "üçµ <b>Serve step</b>: in this final step, the library returns an accelerated version of the user's model in the DL framework of choice, providing a significant boost in performance.\n",
    "The model is optimized by the 4 Speedster blocks shown in the image below. How they work is presented in the library documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46566733-80de-42c6-bbea-7b4da57ef13a",
   "metadata": {},
   "source": [
    "![speed](../imgs/speedsterdoc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c47864-57ac-40e0-9ee9-490dd965699f",
   "metadata": {},
   "source": [
    "<center><small style=\"font-size: 12px;\">Image taken from Speedster documentation.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e5716-4a4c-4cb7-bef9-9c9cd5d79f3c",
   "metadata": {},
   "source": [
    "<h3>üí´ ViT Optimization with Speedster</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5728da-b923-422e-9271-97d0f4f2e413",
   "metadata": {},
   "source": [
    "<h4>üë©‚Äçüî¨ Experiments</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcdd2e3-a7e4-4cee-821f-f0572aeba830",
   "metadata": {},
   "source": [
    "I performed two experiments with Speedster. First, I performed only optimization with techniques that have no impact on model performance. This is achieved by setting the parameter metric_drop_ths=0.\n",
    "\n",
    "Next, I increased the metric_drop threshold to 0.05 so that speedster could also apply techniques that slightly change the accuracy to provide better speedup, such as quantization and compression. The 0.05 value is very low, which means that we expect the accuracy to remain essentially unchanged, as explained in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "493d9696-1e4c-4c36-bd55-1b4dee2f6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2586fcd1-ba44-41b9-98b1-0c5242966477",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 1 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b0ace4-b36a-436b-8099-f736d6002a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(1, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4445d2cc-57f5-480d-90fe-142354dc773e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:20:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 09:20:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 09:20:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.008123621940612794 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:21:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:21:03\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 09:21:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.006738185882568359 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:21:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:21:32\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:21:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:21:33\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 09:21:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0060863494873046875 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:21:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:22:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.004641294479370117 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.0081 sec/batch ‚îÉ 0.0046 sec/batch  ‚îÉ 1.75x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 123.10 data/sec  ‚îÉ 215.46 data/sec   ‚îÉ 1.75x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 346.83 MB         ‚îÉ 0%            ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0                 ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp32              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "Max speed-up with your input parameters is 1.75x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"constrained\",\n",
    "    metric_drop_ths=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6115c8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 11:00:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 11:00:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 11:00:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.008265492916107177 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:00:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:00:35\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 11:00:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0067327022552490234 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:00:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:00:39\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 11:00:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.007616758346557617 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:00:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:01:08\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:01:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:02:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0029878616333007812 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:02:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:06:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0029001235961914062 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:06:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:06:04\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 11:06:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.00616765022277832 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:06:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:06:14\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 11:06:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.003365039825439453 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:06:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:06:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.004647254943847656 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:06:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:07:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0024514198303222656 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:07:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:10:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.004577159881591797 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.0083 sec/batch ‚îÉ 0.0025 sec/batch  ‚îÉ 3.37x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 120.98 data/sec  ‚îÉ 407.93 data/sec   ‚îÉ 3.37x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 174.43 MB         ‚îÉ -49%          ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0.0207            ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp16              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n"
     ]
    }
   ],
   "source": [
    "optimized_model_metric_drop = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"unconstrained\",\n",
    "    metric_drop_ths=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4acaa42-6a26-4a46-8140-9cc7a56fcc33",
   "metadata": {},
   "source": [
    "<h4>Speedster Optimized Model Benchmark</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ae64e-6a1f-46fa-9b4a-563778983568",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5159b437-80e6-4dd8-8536-f832b507fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:22:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 218.85it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:04<00:00, 218.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Average Throughput: 220.19 data/second\n",
      "Average Latency: 0.0045 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927612f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 11:10:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 427.99it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02<00:00, 424.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Average Throughput: 427.80 data/second\n",
      "Average Latency: 0.0023 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model_metric_drop, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3124a9c3-7be0-4c9f-ac6d-3babe0509c21",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae4df44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:23:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on CPU\u001b[0m\n",
      "\u001b[32m2023-02-16 09:23:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 09:24:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.17869094610214234 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:24:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:24:14\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 09:24:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.1728215217590332 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:24:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with DeepSparseCompiler and q_type: None.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.3.2 COMMUNITY | (7d31c4bf) (release) (optimized) (system=avx2, binary=avx2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:24:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.1993265151977539 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:24:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:24:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.15670418739318848 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:24:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: None.\u001b[0m\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpoaccyrgp/fp32/temp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpoaccyrgp/fp32/temp.bin\n",
      "\u001b[32m2023-02-16 09:25:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.1628718376159668 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ ONNXRuntime       ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.1787 sec/batch ‚îÉ 0.1567 sec/batch  ‚îÉ 1.14x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 5.60 data/sec    ‚îÉ 6.38 data/sec     ‚îÉ 1.14x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 346.37 MB         ‚îÉ 0%            ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0                 ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp32              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "Max speed-up with your input parameters is 1.14x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"constrained\",\n",
    "    metric_drop_ths=0,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b61201-f406-47a7-a9e9-952f324de01b",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47530cbb-4d29-48a1-a09f-626d693bb0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:25:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 20 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.10it/s]\n",
      "Performing benchmark on 100 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:16<00:00,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1\n",
      "Average Throughput: 6.31 data/second\n",
      "Average Latency: 0.1584 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, input_data, device=\"cpu\", n_warmup=20, n_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f06a1b-5976-4403-a36c-4180e1f6bc77",
   "metadata": {},
   "source": [
    "After trying several approaches, Speedster selected the use of TensorRT as the technique that best optimizes the model if gpu is used. While ONNXRun when on cpu.\n",
    "\n",
    "With Speedster already with batch size of 1, improvements are achieved in terms of model throughput on gpu. Going from an average Throughput of <b>114.39</b> for the original model, to an average Throughput of <b>220.19</b> for the Speedster optimized model. While on cpu from <b>5.60</b> for the unoptimized model to <b>6.31</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec46c47-3d05-44b0-9ba4-6bdd33b285f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 2 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e56151-509b-4768-9322-3fdbc8c07964",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(2, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aadddcc-ad01-494b-97c7-1b75f98dfd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:25:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 09:25:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 09:25:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.00877812623977661 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:26:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:26:05\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 09:26:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.008505582809448242 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:26:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:26:25\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:26:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:26:26\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 09:26:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.007837057113647461 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:26:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:27:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.00702214241027832 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.0088 sec/batch ‚îÉ 0.0070 sec/batch  ‚îÉ 1.25x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 227.84 data/sec  ‚îÉ 142.41 data/sec   ‚îÉ 1.25x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 346.83 MB         ‚îÉ 0%            ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0                 ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp32              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "Max speed-up with your input parameters is 1.25x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"constrained\",\n",
    "    metric_drop_ths=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac213552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 11:19:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.009010717868804932 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:35\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.008727788925170898 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:36\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0071909427642822266 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:57\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:19:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:20:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0035631656646728516 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:20:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:24:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.003391742706298828 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:24:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:24:39\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 11:24:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.007906675338745117 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:24:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:24:49\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 11:24:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0041713714599609375 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:24:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:25:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.006999492645263672 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:25:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:26:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0030524730682373047 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:26:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:29:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.007112979888916016 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.0090 sec/batch ‚îÉ 0.0031 sec/batch  ‚îÉ 2.95x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 221.96 data/sec  ‚îÉ 327.60 data/sec   ‚îÉ 2.95x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 174.15 MB         ‚îÉ -49%          ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0.0225            ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp16              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n"
     ]
    }
   ],
   "source": [
    "optimized_model_metric_drop = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"unconstrained\",\n",
    "    metric_drop_ths=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b18d8-e8e6-4640-96d3-ef59053fa4eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Speedster Optimized Model Benchmark</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea66bbb-0c41-40af-a57c-7271f2fa2601",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c227b436-74f4-47ee-8edf-f158f62b72e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:27:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 145.93it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:06<00:00, 144.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n",
      "Average Throughput: 290.16 data/second\n",
      "Average Latency: 0.0034 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bd67cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 11:29:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 353.16it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02<00:00, 349.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n",
      "Average Throughput: 704.12 data/second\n",
      "Average Latency: 0.0014 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model_metric_drop, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c0da82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:28:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on CPU\u001b[0m\n",
      "\u001b[32m2023-02-16 09:29:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 09:30:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.3269576716423035 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:30:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:30:09\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 09:30:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.3234107494354248 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:30:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with DeepSparseCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:30:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.3949728012084961 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:30:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:31:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.3019731044769287 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:31:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: None.\u001b[0m\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmp37pxlqou/fp32/temp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmp37pxlqou/fp32/temp.bin\n",
      "\u001b[32m2023-02-16 09:31:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.3071017265319824 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ ONNXRuntime       ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.3270 sec/batch ‚îÉ 0.3020 sec/batch  ‚îÉ 1.08x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 6.12 data/sec    ‚îÉ 3.31 data/sec     ‚îÉ 1.08x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 346.37 MB         ‚îÉ 0%            ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0                 ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp32              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "Max speed-up with your input parameters is 1.08x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"constrained\",\n",
    "    metric_drop_ths=0,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1e4476-9fa0-4821-be94-0f81c7bd742e",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d82e8a-9f9c-443f-86d9-7aed67e822a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:31:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 20 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.20it/s]\n",
      "Performing benchmark on 100 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:31<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n",
      "Average Throughput: 6.49 data/second\n",
      "Average Latency: 0.1542 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, input_data, device=\"cpu\", n_warmup=20, n_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0afc923-1462-4355-8351-bc45008306aa",
   "metadata": {},
   "source": [
    "Again as with the batch size of 1, Speedster selected the use of TensorRT as the technique that best optimizes the model when using gpu. While ONNXRun when on cpu.\n",
    "\n",
    "With batch size equal to 2 the model optimized with Speedster turns out to be faster than the original model on both gpu and cpu. If we accept a loss in prediction accuracy of 5% we get a model with more than 3x speedup over the original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f11ecb-7edd-4998-b8d7-c095b98feadb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 4 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d92be299-5418-4aaa-9b05-dfc85b78fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(4, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c24ad4c-6ec3-4b91-a530-3cdf4e87099a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:32:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 09:32:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 09:32:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.013259525299072266 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:32:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:32:49\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 09:32:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.024447202682495117 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:32:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:33:12\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:33:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:33:12\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 09:33:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.014016389846801758 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:33:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:33:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.013077020645141602 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.0133 sec/batch ‚îÉ 0.0131 sec/batch  ‚îÉ 1.01x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 301.67 data/sec  ‚îÉ 76.47 data/sec    ‚îÉ 1.01x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 346.94 MB         ‚îÉ 0%            ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0                 ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp32              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "Max speed-up with your input parameters is 1.01x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"constrained\",\n",
    "    metric_drop_ths=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef74b427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 11:40:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 11:40:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 11:40:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.013236365318298339 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:40:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:40:56\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 11:40:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.013654947280883789 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:40:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:40:57\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 11:40:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.007341861724853516 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:40:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:41:18\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:41:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:42:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.004583120346069336 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:42:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:45:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.004569530487060547 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:45:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:46:00\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 11:46:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.014111757278442383 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:46:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:46:10\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 11:46:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.005777835845947266 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:46:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:46:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.012988090515136719 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:46:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:47:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.004120349884033203 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 11:47:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-02-16 11:50:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.012813806533813477 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.0132 sec/batch ‚îÉ 0.0041 sec/batch  ‚îÉ 3.21x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 302.20 data/sec  ‚îÉ 242.70 data/sec   ‚îÉ 3.21x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 173.80 MB         ‚îÉ -49%          ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0.0248            ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp16              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n"
     ]
    }
   ],
   "source": [
    "optimized_model_metric_drop = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"unconstrained\",\n",
    "    metric_drop_ths=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b5b2c-442e-4da5-9930-0f5ffe8cc8f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Speedster Optimized Model Benchmark</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3844e5-c647-4b35-9c92-e37e11ef5a2d",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d712bfb2-5beb-4f7c-a75d-6c39b3e9be8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:34:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 78.76it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:12<00:00, 78.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Average Throughput: 315.12 data/second\n",
      "Average Latency: 0.0032 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6d0447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 11:50:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 267.54it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:03<00:00, 262.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Average Throughput: 1060.41 data/second\n",
      "Average Latency: 0.0009 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model_metric_drop, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baa16cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:35:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on CPU\u001b[0m\n",
      "\u001b[32m2023-02-16 09:36:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 09:38:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.5760366940498352 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:38:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:38:13\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 09:38:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.5786547660827637 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:38:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with DeepSparseCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:39:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.8065593242645264 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:39:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:39:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.6088290214538574 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:39:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: None.\u001b[0m\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpvmgx4xik/fp32/temp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpvmgx4xik/fp32/temp.bin\n",
      "\u001b[32m2023-02-16 09:40:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.5567727088928223 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ OpenVINO          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.5760 sec/batch ‚îÉ 0.5568 sec/batch  ‚îÉ 1.03x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 6.94 data/sec    ‚îÉ 1.80 data/sec     ‚îÉ 1.03x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 346.74 MB         ‚îÉ 0%            ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0                 ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp32              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "Max speed-up with your input parameters is 1.03x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"constrained\",\n",
    "    metric_drop_ths=0,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c8689-f696-42b2-99f2-32381ccbf2f5",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "664bb2c9-1a1b-4a7b-9d4d-db0fc44724a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:40:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 10 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.72it/s]\n",
      "Performing benchmark on 15 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:08<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "Average Throughput: 6.89 data/second\n",
      "Average Latency: 0.1452 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, input_data, device=\"cpu\", n_warmup=10, n_runs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf121561-a22f-4980-b645-406fd8889535",
   "metadata": {},
   "source": [
    "Here Speedster selects TensorRT for gpu as the best technique, while OpenVINO for cpu.\n",
    "\n",
    "The results again remain in line as the cases with batch size equal 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a616bc-3124-482e-a85a-f4e7701b00bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 8 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46a5df8e-9fab-49b2-bcf0-49def62aec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(8, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4604dfa-0bb8-4102-ac92-d570f7e6dfb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:40:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 09:40:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 09:41:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.025456199645996092 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:41:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:41:09\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 09:41:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.02623724937438965 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:41:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:41:32\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:41:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:41:33\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 09:41:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.02676844596862793 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:41:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:42:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.024919986724853516 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.0255 sec/batch ‚îÉ 0.0249 sec/batch  ‚îÉ 1.02x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 314.27 data/sec  ‚îÉ 40.13 data/sec    ‚îÉ 1.02x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 346.85 MB         ‚îÉ 0%            ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0                 ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp32              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "Max speed-up with your input parameters is 1.02x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"constrained\",\n",
    "    metric_drop_ths=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08c93139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 12:48:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 12:48:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 12:48:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.025454015731811525 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 12:49:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:49:01\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 12:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.026031970977783203 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 12:49:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:49:03\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 12:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.007582187652587891 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 12:49:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:49:28\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:49:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:50:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.007605552673339844 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 12:50:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:54:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.007861137390136719 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 12:54:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:54:22\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 12:54:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.026823997497558594 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 12:54:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:54:33\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 12:54:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.009075164794921875 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 12:54:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:55:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.025179624557495117 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 12:55:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:55:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.00715327262878418 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 12:55:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-02-16 12:59:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.024901628494262695 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.0255 sec/batch ‚îÉ 0.0072 sec/batch  ‚îÉ 3.56x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 314.29 data/sec  ‚îÉ 139.80 data/sec   ‚îÉ 3.56x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 173.85 MB         ‚îÉ -49%          ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0.0273            ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp16              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n"
     ]
    }
   ],
   "source": [
    "optimized_model_metric_drop = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"unconstrained\",\n",
    "    metric_drop_ths=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2c00b-dff5-4cf2-9f91-6a3ec88f4c96",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Speedster Optimized Model Benchmark</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5263c449-3cad-447d-a3b1-80c80a1e536b",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c91e3945-e119-4cd5-90b0-f91b45f9b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:42:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 41.08it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:24<00:00, 40.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Average Throughput: 328.79 data/second\n",
      "Average Latency: 0.0030 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24437b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 12:59:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 156.99it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:06<00:00, 154.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Average Throughput: 1248.45 data/second\n",
      "Average Latency: 0.0008 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model_metric_drop, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ceb85e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:42:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on CPU\u001b[0m\n",
      "\u001b[32m2023-02-16 09:44:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 09:47:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 1.147692756652832 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:47:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:47:25\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 09:48:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 1.1343605518341064 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:48:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with DeepSparseCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:49:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 1.5942871570587158 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:49:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:50:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 1.2054996490478516 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:50:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: None.\u001b[0m\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmp7r818mt1/fp32/temp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmp7r818mt1/fp32/temp.bin\n",
      "\u001b[32m2023-02-16 09:51:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 1.0849554538726807 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ OpenVINO          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 1.1477 sec/batch ‚îÉ 1.0850 sec/batch  ‚îÉ 1.06x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 6.97 data/sec    ‚îÉ 0.92 data/sec     ‚îÉ 1.06x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 346.75 MB         ‚îÉ 0%            ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0                 ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp32              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "Max speed-up with your input parameters is 1.06x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"constrained\",\n",
    "    metric_drop_ths=0,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9457c81-a6cd-4f45-aff3-995d62ad4b94",
   "metadata": {},
   "source": [
    "<h5>CPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a42a1de-31ae-40c3-96a3-5b7b6119cf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:51:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 10 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:11<00:00,  1.11s/it]\n",
      "Performing benchmark on 15 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:16<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8\n",
      "Average Throughput: 7.22 data/second\n",
      "Average Latency: 0.1386 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, input_data, device=\"cpu\", n_warmup=10, n_runs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564fa54-bc50-468b-944c-ac641100380a",
   "metadata": {},
   "source": [
    "The best approaches are to use TensorRT for GPU and OpenVINO for cpu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8e2a0-216f-43ce-9e42-a0c62dada8a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 32 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f5da44-9135-40e0-aedb-5b11ddddb532",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(32, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "868efcc3-a706-4018-a6fa-a024f9456965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:51:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 09:51:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 09:52:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.09855757713317871 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:52:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:52:21\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 09:52:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.10088324546813965 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:52:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:52:49\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:52:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:52:50\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 09:52:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.1034550666809082 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 09:52:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 09:53:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.09570598602294922 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.0986 sec/batch ‚îÉ 0.0957 sec/batch  ‚îÉ 1.03x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 324.68 data/sec  ‚îÉ 10.45 data/sec    ‚îÉ 1.03x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 346.92 MB         ‚îÉ 0%            ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0                 ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp32              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "Max speed-up with your input parameters is 1.03x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"constrained\",\n",
    "    metric_drop_ths=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b7241fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 13:34:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 13:34:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 13:34:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.09862235069274902 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 13:34:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:34:49\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 13:34:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.10131096839904785 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 13:34:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:34:56\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 13:35:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.028622865676879883 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 13:35:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:35:26\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:35:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:36:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.02461385726928711 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 13:36:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:40:49\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:40:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:40:51\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 13:40:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.10325741767883301 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 13:40:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:41:04\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 13:41:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.029699325561523438 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 13:41:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:41:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.09623026847839355 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 13:41:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:42:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.024158477783203125 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 13:42:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
      "\u001b[32m2023-02-16 13:47:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.09645819664001465 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.0986 sec/batch ‚îÉ 0.0242 sec/batch  ‚îÉ 4.08x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 324.47 data/sec  ‚îÉ 41.39 data/sec    ‚îÉ 4.08x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 174.04 MB         ‚îÉ -49%          ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0.0308            ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp16              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n"
     ]
    }
   ],
   "source": [
    "optimized_model_metric_drop = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"unconstrained\",\n",
    "    metric_drop_ths=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369699e-7b65-4dc2-a5ae-0188178d9cba",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Speedster Optimized Model Benchmark</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee97297-70b6-4fc4-81ae-16a179aa0fe8",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45b4a793-30e7-4591-a59c-8941a9b3e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 09:53:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:04<00:00, 10.72it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:33<00:00, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Average Throughput: 342.10 data/second\n",
      "Average Latency: 0.0029 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac81ff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 13:47:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 46.61it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:21<00:00, 46.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Average Throughput: 1488.99 data/second\n",
      "Average Latency: 0.0007 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model_metric_drop, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19bcb24-43ca-473e-89ef-ec95e903f16e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<h3>Batch Size = 64 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45897447-d29f-4d29-ab96-02258c542eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [((torch.randn(64, 3, 224, 224), ), torch.tensor([0])) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "368dc6ac-85ca-4cf5-ae3e-7d666ed482d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 10:33:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU:0\u001b[0m\n",
      "\u001b[32m2023-02-16 10:33:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
      "\u001b[32m2023-02-16 10:34:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.1909184455871582 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 10:34:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 10:34:19\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
      "\u001b[32m2023-02-16 10:34:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.19694280624389648 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 10:34:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 10:34:57\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
      "\u001b[32m2023-02-16 10:34:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 10:34:58\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mTensorrtExecutionProvider for onnx is not available. If you want to use it, please  add the path to tensorrt to the LD_LIBRARY_PATH environment variable. CUDA provider will be used instead. \u001b[0m\n",
      "\u001b[32m2023-02-16 10:35:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.19941234588623047 sec/iter\u001b[0m\n",
      "\u001b[32m2023-02-16 10:35:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
      "\u001b[32m2023-02-16 10:35:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.1935880184173584 sec/iter\u001b[0m\n",
      "\n",
      "[Speedster results on Tesla V100-SXM2-16GB]\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ Metric      ‚îÉ Original Model   ‚îÉ Optimized Model   ‚îÉ Improvement   ‚îÉ\n",
      "‚î£‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïã‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î´\n",
      "‚îÉ backend     ‚îÉ PYTORCH          ‚îÉ TensorRT          ‚îÉ               ‚îÉ\n",
      "‚îÉ latency     ‚îÉ 0.1909 sec/batch ‚îÉ 0.1936 sec/batch  ‚îÉ 0.99x         ‚îÉ\n",
      "‚îÉ throughput  ‚îÉ 335.22 data/sec  ‚îÉ 5.17 data/sec     ‚îÉ 0.99x         ‚îÉ\n",
      "‚îÉ model size  ‚îÉ 346.34 MB        ‚îÉ 347.02 MB         ‚îÉ 0%            ‚îÉ\n",
      "‚îÉ metric drop ‚îÉ                  ‚îÉ 0                 ‚îÉ               ‚îÉ\n",
      "‚îÉ techniques  ‚îÉ                  ‚îÉ fp32              ‚îÉ               ‚îÉ\n",
      "‚îó‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îª‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îõ\n",
      "\n",
      "Max speed-up with your input parameters is 0.99x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"constrained\",\n",
    "    metric_drop_ths=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eaeec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model_metric_drop = optimize_model(\n",
    "    model, \n",
    "    input_data=input_data, \n",
    "    optimization_time=\"unconstrained\",\n",
    "    metric_drop_ths=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d1505-5224-42fd-9bf4-0c1199ae72eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Speedster Optimized Model Benchmark</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0b89e-713b-49ef-8992-c0c02e814d6a",
   "metadata": {},
   "source": [
    "<h5>GPU</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2c56c47-ef44-4df5-882a-e0c4200438c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 10:35:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:09<00:00,  5.31it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [03:09<00:00,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Average Throughput: 338.72 data/second\n",
      "Average Latency: 0.0030 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f473d02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-02-16 14:17:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing warm up on 50 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:02<00:00, 23.72it/s]\n",
      "Performing benchmark on 1000 iterations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:42<00:00, 23.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Average Throughput: 1513.78 data/second\n",
      "Average Latency: 0.0007 seconds/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(optimized_model_metric_drop, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f550ed1-587d-4f0c-9ef0-0315f615af93",
   "metadata": {},
   "source": [
    "Overall even for high batch sizes Speedster leads to a noticeable acceleration of the model, especially if we accept a drop in the reference metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f536d-bf2c-46d2-8573-6f0796fd9700",
   "metadata": {},
   "source": [
    "<h3>üëæ Results</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f75d6-9a60-42bf-b661-fbbd042b31e7",
   "metadata": {},
   "source": [
    "Let's analyze the results:\n",
    "\n",
    "* Speedster is easy to use and allows information to be obtained during optimization regarding the techniques tested.\n",
    "* The library makes it possible to display initial metrics related to model acceleration without the need to run the benchmark function.\n",
    "* Using half precision, flash attention and hardware-specific compilation techniques, Speedster on GPU can significantly accelerate the model with a target metric lower than 0.05 compared to the original version. The default metric used by speedster is the numeric precision, which measures the average relative difference between the original model and the optimized one. Therefore, any model with a target metric lower than 0.05 can be considered as having no accuracy loss.\n",
    "* The original model and Speedster had similar throughput on CPU. Increasing the metric drop to 0.05 did not help with speed-up. This is because many layers in Speedster do not have fp16 kernels, so converting fp32 tensors to fp16 and back inside the network slows down latency. Also, int8 conversion for the ViT model made numeric precision drop, and did not meet the 0.05 constraint. This makes sense because transformers are more affected by quantization, due to having \"outliers\" in the activations. Using QAT before quantizing, i.e. fine-tuning the model with simulated quantization, can then be used for getting a better speed-up with Speedster on CPUs. One way to improve Speedster performance on CPUs might be to implement ToMe within the library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa622104-c2d8-4647-9517-2d45eeb3e51a",
   "metadata": {},
   "source": [
    "![speedsterresults](../imgs/speedster-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b281b-40e1-46fe-91a4-93a664b13b61",
   "metadata": {},
   "source": [
    "<center><small style=\"font-size: 12px;\">Throughput graph for the original model and the model to which Speedster was applied, as the batch size varies.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98424b-aec5-419c-9728-f929d50c0657",
   "metadata": {},
   "source": [
    "<a id=\"play\"></a>\n",
    "<h2>üé® Test ToMe With Your Own Picture</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d374e65-6b36-4f40-944a-479b269dfa93",
   "metadata": {},
   "source": [
    "In this section you can find a section where you can test ToMe on your images, with the possibility of changing the hyperparameter r that adjusts the level of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113d18b-3b3d-4265-a9ce-34edadaba9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this with the path of your image\n",
    "PATH_TO_IMAGE = \"imgs/PATH_TO_YOUR_IMAGE.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f656a-f652-4d7f-a189-d3fd2129e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d03f71-68b4-4935-917f-f1cd39252278",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c6565-458c-4d38-a0b0-2963df83f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vit_large_patch16_384\"\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "tome.patch.timm(model, trace_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2042e-ed7b-4c80-9036-642565641321",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = model.default_cfg[\"input_size\"][1]\n",
    "\n",
    "transform_list = [\n",
    "    transforms.Resize(int((256 / 224) * input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(input_size)\n",
    "]\n",
    "\n",
    "transform_vis  = transforms.Compose(transform_list)\n",
    "transform_norm = transforms.Compose(transform_list + [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(model.default_cfg[\"mean\"], model.default_cfg[\"std\"]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5334e-c07c-4cf0-978a-ffbbfe8a204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(PATH_TO_IMAGE).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9ec43-ea62-42d2-b1dd-e439f65b6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vis = transform_vis(img)\n",
    "img_norm = transform_norm(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f13e0-30b9-40cf-af38-54c116fc00c1",
   "metadata": {},
   "source": [
    "You can change the hyper-parameter <em>r</em>. The larger <em>r</em> is, the more pixels in your image will be merged together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a3e7f-a4ec-4ffb-9b1e-03af78456108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.r = 16\n",
    "_ = model(img_norm[None, ...])\n",
    "source = model._tome_info[\"source\"]\n",
    "\n",
    "print(f\"{source.shape[1]} tokens at the end\")\n",
    "tome.make_visualization(img_vis, source, patch_size=16, class_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d83135-873f-471d-acc7-bca813e206e4",
   "metadata": {},
   "source": [
    "The results as r varies will be similar to these:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5053769-2a23-49fa-a421-c5edba57ad4d",
   "metadata": {},
   "source": [
    "![thanos](../imgs/thanos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b4b97-ee62-464a-a958-02636dac6b9d",
   "metadata": {},
   "source": [
    "<center><small style=\"font-size: 12px;\">ToMe test on a picture of me.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26713c7-8582-4158-bcb2-576725ae44fb",
   "metadata": {},
   "source": [
    "Yes I admit, unfortunately the images will be without Thanos :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bf9f8-c1b3-4a34-923e-d6671b00674d",
   "metadata": {},
   "source": [
    "<h2>üåà Conclusions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397cb092-f4ba-480a-8f49-0246deef0839",
   "metadata": {},
   "source": [
    "ToMe makes it possible to accelerate Visual Transformer models, both on GPU and CPU. One interesting thing to notice is that ToMe improves the model's speed on CPU inference, but reduces it on GPU when the batch size is low. This can be explained by the fact that CPU uses its full compute power for smaller batch sizes, while GPU has more room for parallel computation. Therefore, ToMe's overhead on CPU is offset by the token reduction, but not on GPU until the batch size is large enough. This can also be seen from the graphs below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56a53-3027-4b8b-8c02-0fbac0be7d55",
   "metadata": {},
   "source": [
    "![totalresults](../imgs/total-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7aa9e3-d322-4ba7-a7c1-f39663fee7eb",
   "metadata": {},
   "source": [
    "<center><small style=\"font-size: 12px;\">Results obtained with different optimization techniques, with various values for batch size.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5a2bf-c366-49ed-bb5d-a680917303c2",
   "metadata": {},
   "source": [
    "Here we can see that Speedster accepting a 5% performance loss is significantly faster than the original model, remembering that ToMe also leads to performance losses the comparison between the techniques can be considered fair. While on the CPU ToMe appears to be the fastest technique, so it might be interesting to implement its automatic use within Speedster. I opened an [issue on Speedster GitHub](https://github.com/nebuly-ai/nebullvm/issues/174) so that anyone can contribute.\n",
    "And that's it! If you are interested in AI optimization or if you liked this notebook please leave a star at our repo [Speedster](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster) üíïüåü!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c2469-b23a-4572-a80d-11e930a3fee8",
   "metadata": {},
   "source": [
    "Thanks to the Nebuly team for their support in these analyses ‚ù§Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1513b1b-0c9e-4829-a0bd-ba1e9695bb94",
   "metadata": {},
   "source": [
    "<h2>üíæ References</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c5336e-3e04-4c82-8ae9-631f4cfdc95d",
   "metadata": {},
   "source": [
    "* [Speedster](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster)\n",
    "* [Blog Post: Token Merging: Your ViT but faster](https://research.facebook.com/blog/2023/2/token-merging-your-vit-but-faster/?utm_source=linkedin&utm_medium=organic_social&utm_campaign=evergreen&utm_content=animation)\n",
    "* [Paper: Token Merging: Your ViT but faster](https://arxiv.org/pdf/2210.09461.pdf)\n",
    "* [Timm](https://github.com/rwightman/pytorch-image-models#getting-started-documentation)\n",
    "* [Nebuly](https://www.nebuly.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33010e-2148-4abf-acc7-46d76d39d99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
